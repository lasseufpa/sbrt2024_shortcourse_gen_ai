{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBRT 2024 - **An Introduction to Generative Artificial Intelligence with Applications in Telecommunications**\n",
    "\n",
    "### This python notebook was based in the code at https://github.com/benediktfesl/Diffusion_channel_est, that comes from the paper called: [Diffusion-based Generative Prior for Low-Complexity MIMO Channel Estimation](https://arxiv.org/abs/2403.03545)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Heads-up:*** this notebook is only pedagogical. In other words, it is meant to showcase code snippets from the original repository that highlight the steps to generate a channel estimate using a diffusion model, but without considering the possibility to execute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) This notebook is based on the script `load_and_eval_dm.py`, available at https://github.com/benediktfesl/Diffusion_channel_est/blob/master/load_and_eval_dm.py\n",
    "\n",
    "2) The `modules` imported in the cell below is in the folder https://github.com/benediktfesl/Diffusion_channel_est/tree/master/modules\n",
    "\n",
    "3) The `DMCE` imported in the cell below is in the folder https://github.com/benediktfesl/Diffusion_channel_est/tree/master/DMCE\n",
    "\n",
    "4) To run this and other scripts from this repository it is also necessary to download the channel data from https://syncandshare.lrz.de/getlink/fi93y1AnwmsvHrAGNqq5zX/ (password: `Diffusion2024`) and move it into a folder named `bin`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train and test script for the DMCE.\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import modules.utils as ut\n",
    "import datetime\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import DMCE\n",
    "import torch\n",
    "from DMCE.utils import cmplx2real\n",
    "\n",
    "CUDA_DEFAULT_ID = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "date_time_now = datetime.datetime.now()\n",
    "date_time = date_time_now.strftime('%Y-%m-%d_%H-%M-%S')  # convert to str compatible with all OSs\n",
    "\n",
    "n_dim = 64 # RX antennas\n",
    "n_dim2 = 16 # TX antennas\n",
    "num_train_samples = 100_000\n",
    "num_val_samples = 10_000  # must not exceed size of training set\n",
    "num_test_samples = 10_000\n",
    "\n",
    "return_all_timesteps = False # evaluates all intermediate MSEs\n",
    "fft_pre = True # learn channel distribution in angular domain through Fourier transform\n",
    "reverse_add_random = False # re-sampling in the reverse process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data params\n",
    "ch_type = '3gpp' # {quadriga_LOS, 3gpp}\n",
    "n_path = 3\n",
    "if n_dim2 > 1:\n",
    "    mode = '2D'\n",
    "else:\n",
    "    mode = '1D'\n",
    "\n",
    "_, _, data_test = ut.load_or_create_data(ch_type=ch_type, n_path=n_path, n_antennas_rx=n_dim,\n",
    "                                                         n_antennas_tx=n_dim2, n_train_ch=num_train_samples,\n",
    "                                                         n_val_ch=num_val_samples,\n",
    "                                                         n_test_ch=num_test_samples, return_toep=False)\n",
    "del _\n",
    "if ch_type.startswith('3gpp') and n_dim2 > 1:\n",
    "    data_test = np.reshape(data_test, (-1, n_dim, n_dim2), 'F')\n",
    "data_test = torch.from_numpy(np.asarray(data_test[:, None, :]))\n",
    "data_test = cmplx2real(data_test, dim=1, new_dim=False).float()\n",
    "if ch_type.startswith('3gpp'):\n",
    "    ch_type += f'_path={n_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model parameter dictionaries\n",
    "cwd = os.getcwd()\n",
    "#which_dataset = dataset\n",
    "model_dir = os.path.join(cwd, './results/best_models_dm_paper', ch_type)\n",
    "sim_params = DMCE.utils.load_params(os.path.join(model_dir, 'sim_params'))\n",
    "num_timesteps = sim_params['diff_model_dict']['num_timesteps']\n",
    "cnn_dict = sim_params['unet_dict']\n",
    "diff_model_dict = sim_params['diff_model_dict']\n",
    "\n",
    "# manually set the correct device for this simulation\n",
    "cnn_dict['device'] = device\n",
    "\n",
    "# instantiate the neural network\n",
    "cnn = DMCE.CNN(**cnn_dict)\n",
    "\n",
    "# instantiate the diffusion model and give it a reference to the unet model\n",
    "diffusion_model = DMCE.DiffusionModel(cnn, **diff_model_dict)\n",
    "\n",
    "# load the parameters of the pre-trained model into the DiffusionModel instance\n",
    "model_path = os.path.join(model_dir, 'train_models')\n",
    "model_list = os.listdir(model_path)\n",
    "model_path = os.path.join(model_path, model_list[-1])\n",
    "model_params = torch.load(model_path, map_location=device)\n",
    "\n",
    "diffusion_model.load_state_dict(model_params['model'])\n",
    "\n",
    "diffusion_model.reverse_add_random = reverse_add_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester parameter dictionary, which is saved in 'sim_params.json'\n",
    "tester_dict = {\n",
    "    'batch_size': 512,\n",
    "    'criteria': ['nmse'],\n",
    "    'complex_data': False,\n",
    "    'return_all_timesteps': return_all_timesteps,\n",
    "    'fft_pre': fft_pre,\n",
    "    'mode': mode,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the Tester and give it a reference to the diffusion model as well as testing data\n",
    "tester = DMCE.Tester(diffusion_model, data=data_test, **tester_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the test() function. This returns a dictionary with the testing stats.\n",
    "# Depending on the size of the test set, this might take a while.\n",
    "test_dict = tester.test()\n",
    "\n",
    "# to store the results\n",
    "os.makedirs('./results/dm_est/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing `tester.test()`, the code below is called. There, we can observe the steps mentioned in the slides for the channel estimation phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the step 3, the first one in the channel estimation phase from the slides. This happens in the line `29`\n",
    "\n",
    "`y = functional.awgn(data_batch, snr, multiplier=self.model.noise_multiplier)`\n",
    "\n",
    "where an initial estimate of the channel is obtained by implementing \n",
    "\n",
    "<img src=\"./figures/step3.png\" width=240 height=40 />\n",
    "\n",
    "which in code, translates to:\n",
    "\n",
    "`x + (1 / snr ** 0.5) * multiplier * torch.randn_like(x)`\n",
    "\n",
    "where x is the input matrix data. So, it could be rewritten as \n",
    "\n",
    "`H + (1 / snr ** 0.5) * multiplier * torch.randn_like(H)`\n",
    "\n",
    "with `Ã‘` being\n",
    "\n",
    "`(1 / snr ** 0.5) * torch.randn_like(H)`\n",
    "\n",
    "This adds to `H` an AWGN with variance = `1/snr` (i.e std_variation = `1/snr^0.5`)\n",
    "\n",
    "obs: as mentioned in the code, the `multiplier` is a value added if \"the input data is complex but real and imaginary parts are split up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_nmse(self) -> dict:\n",
    "    \"\"\"\n",
    "    Test function for the NMSE criterion. For different SNR values between -20 and 40 dB, the test data is corrupted\n",
    "    with noise and the DiffusionModel estimates the original data from the noisy input. For each SNR value, the MSE\n",
    "    normalized by the average power of the whole dataset is calculated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_dict: dict\n",
    "        Dictionary with tested SNRs in dB, MSEs normalized per sample and MSEs normalized by the average data power\n",
    "    \"\"\"\n",
    "\n",
    "    # specify which SNRs should be evaluated\n",
    "    snr_db_range = torch.arange(-10, 45, 5, dtype=torch.float32, device=self.device)\n",
    "    snr_range = 10 ** (snr_db_range / 10)\n",
    "\n",
    "    nmse_total_power_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for snr in tqdm(iterable=snr_range):\n",
    "            # test each SNR value\n",
    "            x_hat = []\n",
    "            for data_batch in self.dataloader:\n",
    "                data_batch = data_batch.to(device=self.device)\n",
    "\n",
    "                # NOTE from the workshop team: the code below implements the \n",
    "                # step 3. The first one in the channel estimation phase from the slides\n",
    "                # add noise to the test data\n",
    "                y = functional.awgn(data_batch, snr, multiplier=self.model.noise_multiplier)\n",
    "\n",
    "                # calculate channel estimate\n",
    "                x_est = self.model.generate_estimate(y.to(device=self.device), snr, return_all_timesteps=self.return_all_timesteps)\n",
    "                if self.fft_pre:\n",
    "                    x_est = ut.complex_1d_fft(x_est, ifft=True, mode=self.mode)\n",
    "                x_hat.append(x_est)\n",
    "            x_hat = torch.cat(x_hat, dim=0).cpu()\n",
    "\n",
    "            if len(self.data.shape) == 4:\n",
    "                #print('Reshaping...')\n",
    "                dim = int(self.data.shape[-1] * self.data.shape[-2])\n",
    "                x_hat = ut.reshape_fortran(x_hat, (-1, dim))\n",
    "                nmse_total_power_list.append(functional.nmse_torch(ut.reshape_fortran(torch.squeeze(self.data), (-1, dim)), x_hat, norm_per_sample=False))\n",
    "            else:\n",
    "                # calculate NMSE from estimated channels\n",
    "                nmse_total_power_list.append(functional.nmse_torch(torch.squeeze(self.data), torch.squeeze(x_hat), norm_per_sample=False))\n",
    "\n",
    "    return {'SNRs': snr_db_range.tolist(),\n",
    "            'NMSEs_total_power': nmse_total_power_list,\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the above code, `generate_estimate()` is called, passing the noisy input data `y` and the known current experienced SNR `snr`. There, the diffusion model step `t` is calculated (as shown in the slides, in the step 6) and used in the function `reverse_sample_loop()` to obtain the channel estimate `x_hat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, in the code, the step 6 comes first, estimating the diffusion model step\n",
    "\n",
    "<img src=\"./figures/step6.png\" width=300 height=40 />.\n",
    "\n",
    "After that, the data is normalized in step 4:\n",
    "\n",
    "<img src=\"./figures/step4.png\" width=240 height=40 />\n",
    "\n",
    "and the DM reverse process is initiallized (step 7):\n",
    "\n",
    "<img src=\"./figures/step7.png\" width=120 height=40 />\n",
    "\n",
    "Let's recall the steps until now:\n",
    "\n",
    "<img src=\"./figures/slide.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_estimate(self, y: torch.Tensor, snr: float, *, add_random: bool = None,\n",
    "                          return_all_timesteps: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Implements the estimation algorithm for channel data, but can also be used for other data types. Requires the DM\n",
    "        to already be trained in order to work properly. It scales the input and performs the reverse process starting\n",
    "        at the timestep that corresponds to the correct SNR value. Intended for public use.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : Tensor of shape [batch_size, *self.data_shape]\n",
    "            batch_size noisy data samples\n",
    "        snr : float\n",
    "            Estimated or known SNR of the noisy data sample\n",
    "        return_all_timesteps : optional bool\n",
    "            specifies whether to return the data samples of all timesteps or only the final one.\n",
    "        add_random : optional bool\n",
    "            Specifies whether the reverse_step should be deterministic or include a noise sampling step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_hat : Tensor of shape [n_samples, *self.data_shape]\n",
    "            The denoised data samples after the whole reverse process\n",
    "        OR\n",
    "        x_ts : Tensor of shape [t_start + 1, n_samples, *self.data_shape]\n",
    "            Collection of the  data samples in all timesteps. x_ts[-1] contains the fully denoised data samples.\n",
    "        \"\"\"\n",
    "\n",
    "        add_random = utils.default(add_random, self.reverse_add_random)\n",
    "\n",
    "        # NOTE from the workshop team: notice the step 6 happening below:\n",
    "        # estimate t_hat, the time step that corresponds to the correct SNR\n",
    "        t = int(torch.abs(self.snrs - snr).argmin())\n",
    "\n",
    "        # NOTE from the workshop team: the code below implements the step 4:\n",
    "        # normalize the input data accordingly (this might differ for other data than normalized channels)\n",
    "        norm_multiplier = (snr / (1 + snr)) ** 0.5\n",
    "\n",
    "        # We consider snr = 1/n2, so the code above can be read as\n",
    "        # (1/((1/n2) / (1 + (1/n2)))) ** -0.5\n",
    "\n",
    "        # (1/(snr / (1 + snr))) ** -0.5\n",
    "\n",
    "                # Taking only the above denominator\n",
    "                #    (1/n2)                  1\n",
    "                # --------------   ->  -------------- \n",
    "                #  (1 + (1/n2))          n2  +  1 \n",
    "\n",
    "        # (1/(1 / (n2 + 1))) ** -0.5\n",
    "\n",
    "                # Taking only the term being exponentiated\n",
    "                #       1                   \n",
    "                # --------------   ->   n2 + 1\n",
    "                #  (1 / (n2 + 1))        \n",
    "\n",
    "        # n2 + 1 ** -0.5 (achieving the equation shown in step 4)\n",
    "\n",
    "        # NOTE from the workshop team: the code below implements the step 7:\n",
    "        x_t = norm_multiplier * y \n",
    "\n",
    "        # NOTE from the workshop team: the function reverse_sample_loop\n",
    "        # implements the step 8:\n",
    "        x_hat = self.reverse_sample_loop(x_t, t, return_all_timesteps=return_all_timesteps, add_random=add_random)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there is the code equivalent to the step 8 from the channel estimation algorithm shown in the paper and the slides: \n",
    "\n",
    "Where, the step 8 is the DM loop:\n",
    "\n",
    "<img src=\"./figures/step8.png\" width=240 height=80 />\n",
    "\n",
    "obtaining the final estimate generated by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_sample_loop(self, x_t: torch.Tensor, t_start: int,\n",
    "                            *, return_all_timesteps: bool = False, add_random: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Implements the whole reverse process down to t=0 by iteratively calling 'reverse_step()'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_t : Tensor of shape [batch_size, *self.data_shape]\n",
    "            batch_size different data samples\n",
    "        t_start : int\n",
    "            starting time step of the reverse process\n",
    "        return_all_timesteps : optional bool\n",
    "            specifies whether to return the data samples of all timesteps or only the final one.\n",
    "        add_random : optional bool\n",
    "            Specifies whether the reverse_step should be deterministic or include a noise sampling step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x_0 : Tensor of shape [batch_size, *self.data_shape]\n",
    "            The denoised data sample after the whole reverse process\n",
    "        OR\n",
    "        x_ts : Tensor of shape [t_start + 1, batch_size, *self.data_shape]\n",
    "            Collection of data samples in all timesteps. x_ts[-1] contains the fully denoised data sample.\n",
    "        \"\"\"\n",
    "\n",
    "        assert t_start <= self.num_timesteps\n",
    "        assert utils.equal_iterables(x_t.shape[1:], self.data_shape)\n",
    "        x_all = [x_t]\n",
    "\n",
    "        # NOTE from the workshop team: the code below implements the loop seem in step 8:\n",
    "        for t in reversed(range(t_start)):\n",
    "            x_t = self.reverse_step(x_t, t, add_random=add_random)\n",
    "            if return_all_timesteps:\n",
    "                x_all.append(x_t)\n",
    "\n",
    "        # clip the final samples for image data to the range [-1, 1]\n",
    "        if self.clipping:\n",
    "            x_all = [torch.clamp(x, -1, 1) for x in x_all]\n",
    "            x_t = torch.clamp(x_t, -1, 1)\n",
    "        if return_all_timesteps:\n",
    "            return torch.stack(x_all, dim=1)\n",
    "        else:\n",
    "            return x_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
